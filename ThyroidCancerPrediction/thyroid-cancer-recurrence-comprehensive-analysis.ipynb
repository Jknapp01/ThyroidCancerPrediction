{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8432491,"sourceType":"datasetVersion","datasetId":5022080}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Necessary Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix as cm\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.naive_bayes import GaussianNB , BernoulliNB , MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier , DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier ,RandomForestRegressor , AdaBoostRegressor\nfrom xgboost import XGBClassifier , XGBRegressor\nfrom sklearn.linear_model import LinearRegression ,LogisticRegression\nfrom sklearn.neighbors import KNeighborsRegressor , KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor , GradientBoostingClassifier\nfrom sklearn.svm import SVC , SVR\nfrom sklearn.model_selection import GridSearchCV , RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error , mean_absolute_error , r2_score , classification_report , accuracy_score , f1_score , precision_score\nimport pickle\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-23T20:04:15.301287Z","iopub.execute_input":"2024-05-23T20:04:15.301674Z","iopub.status.idle":"2024-05-23T20:04:16.612330Z","shell.execute_reply.started":"2024-05-23T20:04:15.301636Z","shell.execute_reply":"2024-05-23T20:04:16.611213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read the CSV file into Pandas DataFrame","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/thyroid-cancer-recurrence-prediction/Thyroid_Diff.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.618326Z","iopub.execute_input":"2024-05-23T20:04:16.618756Z","iopub.status.idle":"2024-05-23T20:04:16.636442Z","shell.execute_reply.started":"2024-05-23T20:04:16.618715Z","shell.execute_reply":"2024-05-23T20:04:16.635249Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summary","metadata":{}},{"cell_type":"markdown","source":"This notebook aims to predict the recurrence of well differentiated Thyroid cancer. \n\n**If this notebook is helpful for you, please consider upvoting 🙏**\n\nWhat is Thyroid cancer? \n\n**Definition:** *Thyroid cancer is a type of malignancy that originates in the thyroid gland, a small butterfly-shaped gland located at the base of the neck. This cancer can present in various forms, including papillary, follicular, medullary, and anaplastic thyroid cancer, each with distinct characteristics and levels of aggressiveness. Papillary thyroid cancer is the most common and typically has a favorable prognosis. Symptoms of thyroid cancer may include a lump in the neck, trouble swallowing, hoarseness, and swollen lymph nodes. Risk factors encompass exposure to radiation, a family history of thyroid cancer, and certain genetic conditions. Treatment often involves a combination of surgery, radioactive iodine therapy, and, in some cases, thyroid hormone therapy or targeted drug therapy. Early detection and advances in treatment have significantly improved the prognosis for most thyroid cancer patients.*\n\nWork like this serves to increase the chances of early detection of recurrence. It is important for patients to understand the likelihood of recurrence so they can adjust their lifestyle and follow-up with their healthcare provider accordingly.\n\n**Some information about the dataset:**\n\nDataset Characteristics: Tabular\\\nFeature Type: Real, Categorical, Integer\\\nFeatures: 16\\\nAssociated Tasks: Classification\n\n**Variables:**\n* Age: Feature, Integer\n* Gender: Feature, Categorical\n* Smoking: Feature, Categorical\n* Hx Smoking: Feature, Categorical\n* Hx Radiothreapy: Feature, Categorical\n* Thyroid Function: Feature, Categorical\n* Physical Examination: Feature, Categorical\n* Adenopathy: Feature, Categorical\n* Pathology: Feature, Categorical\n* Focality: Feature, Categorical\n* Risk: Feature, Categorical\n* T: Feature, Categorical\n* N: Feature, Categorical\n* M: Feature, Categorical\n* Stage: Feature, Categorical\n* Response: Feature, Categorical\n\n**Target Variable:**\n* Recurred: Target, Categorical.","metadata":{}},{"cell_type":"markdown","source":"# 1. Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Datatypes and a Cursory glance at Data","metadata":{}},{"cell_type":"markdown","source":"I want to confirm what the dataset provider asserts regarding the variables:","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.638427Z","iopub.execute_input":"2024-05-23T20:04:16.638798Z","iopub.status.idle":"2024-05-23T20:04:16.655807Z","shell.execute_reply.started":"2024-05-23T20:04:16.638769Z","shell.execute_reply":"2024-05-23T20:04:16.654602Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This seems to match up with what the dataset summary asserts: There is 1 numerical feature and the rest are categorical. There are no null values in the dataset to address in the pre-processing stage. It looks like a relatively small dataset (383 entries) and also a clean dataset (no null). This is a good starting block for exploratory data analysis.\n\nI'll take a look at the first and last 5 rows of the dataset before proceeding.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.659647Z","iopub.execute_input":"2024-05-23T20:04:16.660051Z","iopub.status.idle":"2024-05-23T20:04:16.687334Z","shell.execute_reply.started":"2024-05-23T20:04:16.660014Z","shell.execute_reply":"2024-05-23T20:04:16.686205Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.688842Z","iopub.execute_input":"2024-05-23T20:04:16.689234Z","iopub.status.idle":"2024-05-23T20:04:16.712106Z","shell.execute_reply.started":"2024-05-23T20:04:16.689201Z","shell.execute_reply":"2024-05-23T20:04:16.711054Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I get a pretty good idea of the dataset from examining these 10 rows. Aside from age, all of the features that predict the target variable are categorical as we confirmed earlier, and the target variable is a yes/no categorical variable.","metadata":{"execution":{"iopub.status.busy":"2024-05-22T19:21:04.891127Z","iopub.execute_input":"2024-05-22T19:21:04.891614Z","iopub.status.idle":"2024-05-22T19:21:04.899633Z","shell.execute_reply.started":"2024-05-22T19:21:04.891578Z","shell.execute_reply":"2024-05-22T19:21:04.898323Z"}}},{"cell_type":"markdown","source":"I'll separate the dataset into numerical (age) and categorical features which I may reference later during feature engineering.","metadata":{}},{"cell_type":"markdown","source":"## 1.2 Categorical Features Preprocessing","metadata":{}},{"cell_type":"code","source":"# find categorical and numerical variables  \ncolumns = list(df.columns)\n\ncategoric_columns = []\nnumeric_columns = []\n\nfor i in columns:\n    if len(df[i].unique()) > 8:\n        numeric_columns.append(i)\n    else:\n        categoric_columns.append(i)\n\ncategoric_columns = categoric_columns[:-1] # Excluding target:'Recurred'","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.713543Z","iopub.execute_input":"2024-05-23T20:04:16.713871Z","iopub.status.idle":"2024-05-23T20:04:16.723781Z","shell.execute_reply.started":"2024-05-23T20:04:16.713845Z","shell.execute_reply":"2024-05-23T20:04:16.722558Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Numeric columns: ',numeric_columns)\nprint('Categoric columns: ',categoric_columns)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.725438Z","iopub.execute_input":"2024-05-23T20:04:16.725906Z","iopub.status.idle":"2024-05-23T20:04:16.736718Z","shell.execute_reply.started":"2024-05-23T20:04:16.725864Z","shell.execute_reply":"2024-05-23T20:04:16.735076Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I want to know how many variables there are in my categorical columns so I can determine whether to use a label encoder or one-hot encoding for my features:","metadata":{}},{"cell_type":"code","source":"print(df.nunique())","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.738615Z","iopub.execute_input":"2024-05-23T20:04:16.738996Z","iopub.status.idle":"2024-05-23T20:04:16.757484Z","shell.execute_reply.started":"2024-05-23T20:04:16.738939Z","shell.execute_reply":"2024-05-23T20:04:16.756335Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I can see that the feature with the most number of unique variables is 'T' with 7. I'll try label encoding for this dataset and if I run into any trouble I'll pivot to one-hot encoding for the features with the most unique variables.","metadata":{}},{"cell_type":"markdown","source":"# 2. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Split the Dataset into Train and Test Sets","metadata":{}},{"cell_type":"markdown","source":"There is only 1 numerical variable, so we need to encode the categorical variables before we can proceed towards building any type of machine learning model. Before I do that, I need to separate the target variable from the dataset and split the dataset into training and testing sets.","metadata":{}},{"cell_type":"code","source":"# Assign X and y to predictors and target variable\n\nX = df.drop('Recurred', axis=1)\ny = df[['Recurred']]\n\nprint(X.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.758878Z","iopub.execute_input":"2024-05-23T20:04:16.759279Z","iopub.status.idle":"2024-05-23T20:04:16.767758Z","shell.execute_reply.started":"2024-05-23T20:04:16.759248Z","shell.execute_reply":"2024-05-23T20:04:16.766537Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.769250Z","iopub.execute_input":"2024-05-23T20:04:16.769583Z","iopub.status.idle":"2024-05-23T20:04:16.783302Z","shell.execute_reply.started":"2024-05-23T20:04:16.769555Z","shell.execute_reply":"2024-05-23T20:04:16.782075Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.786592Z","iopub.execute_input":"2024-05-23T20:04:16.787088Z","iopub.status.idle":"2024-05-23T20:04:16.811316Z","shell.execute_reply.started":"2024-05-23T20:04:16.787046Z","shell.execute_reply":"2024-05-23T20:04:16.810044Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.813430Z","iopub.execute_input":"2024-05-23T20:04:16.814256Z","iopub.status.idle":"2024-05-23T20:04:16.826628Z","shell.execute_reply.started":"2024-05-23T20:04:16.814219Z","shell.execute_reply":"2024-05-23T20:04:16.825425Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.832629Z","iopub.execute_input":"2024-05-23T20:04:16.833111Z","iopub.status.idle":"2024-05-23T20:04:16.840998Z","shell.execute_reply.started":"2024-05-23T20:04:16.833078Z","shell.execute_reply":"2024-05-23T20:04:16.839734Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.842702Z","iopub.execute_input":"2024-05-23T20:04:16.843182Z","iopub.status.idle":"2024-05-23T20:04:16.853371Z","shell.execute_reply.started":"2024-05-23T20:04:16.843122Z","shell.execute_reply":"2024-05-23T20:04:16.852182Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The shape of my training sets matches, this is a sign that the split went correclty and I can proceed to feature scaling.","metadata":{}},{"cell_type":"markdown","source":"## 2.2 Feature Scaling","metadata":{}},{"cell_type":"markdown","source":"I'll use a standard scaler on the numerical column (age) before I encode the categorical columns","metadata":{}},{"cell_type":"code","source":"Standard_Scaler = StandardScaler()\nX_train[numeric_columns] = Standard_Scaler.fit_transform(X_train[numeric_columns])\nX_test[numeric_columns] = Standard_Scaler.transform(X_test[numeric_columns])","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.854717Z","iopub.execute_input":"2024-05-23T20:04:16.855067Z","iopub.status.idle":"2024-05-23T20:04:16.872654Z","shell.execute_reply.started":"2024-05-23T20:04:16.855038Z","shell.execute_reply":"2024-05-23T20:04:16.871603Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Encoding Categorical Features","metadata":{}},{"cell_type":"markdown","source":"I need to encode the categorical features before I compare classification models. I'll use Label Encoder to do this, although I think on this dataset One-Hot Encoding is totally suitable as well.","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(y_train)#we use labelencoder to convert categorical data to numeric value\nobject_columns = X_train.select_dtypes(include=['object']).columns\nfor column in object_columns:\n    X_train[column] = label_encoder.fit_transform(X_train[column])#we use labelencoder to convert categorical data to numeric value\n    \ny_test = label_encoder.fit_transform(y_test)#we use labelencoder to convert categorical data to numeric value\nobject_columns = X_test.select_dtypes(include=['object']).columns\nfor column in object_columns:\n    X_test[column] = label_encoder.fit_transform(X_test[column])#we use labelencoder to convert categorical data to numeric value","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.874085Z","iopub.execute_input":"2024-05-23T20:04:16.874578Z","iopub.status.idle":"2024-05-23T20:04:16.906704Z","shell.execute_reply.started":"2024-05-23T20:04:16.874539Z","shell.execute_reply":"2024-05-23T20:04:16.905670Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.907981Z","iopub.execute_input":"2024-05-23T20:04:16.908375Z","iopub.status.idle":"2024-05-23T20:04:16.931369Z","shell.execute_reply.started":"2024-05-23T20:04:16.908341Z","shell.execute_reply":"2024-05-23T20:04:16.930220Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is encoded correctly. We can see this above, all of the object datatypes have been converted to int.","metadata":{}},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.932811Z","iopub.execute_input":"2024-05-23T20:04:16.933204Z","iopub.status.idle":"2024-05-23T20:04:16.944859Z","shell.execute_reply.started":"2024-05-23T20:04:16.933172Z","shell.execute_reply":"2024-05-23T20:04:16.943410Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.946552Z","iopub.execute_input":"2024-05-23T20:04:16.947456Z","iopub.status.idle":"2024-05-23T20:04:16.958454Z","shell.execute_reply.started":"2024-05-23T20:04:16.947415Z","shell.execute_reply":"2024-05-23T20:04:16.957337Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4 Feature Importance","metadata":{}},{"cell_type":"markdown","source":"I want to know what features are most important for predicting the target variable in this dataset. There are two ways to approach this, and I'll apply both to get the full picture: Feature Importance scores as well as Permutation Importance. ","metadata":{}},{"cell_type":"code","source":"# create the classifier with n_estimators = 100\nclf = RandomForestClassifier(n_estimators=100, random_state=0)\n\n# fit the model to the training set\nclf.fit(X_train, y_train)\n\n# view the feature scores\nfeature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n\nfeature_scores","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:16.959912Z","iopub.execute_input":"2024-05-23T20:04:16.960306Z","iopub.status.idle":"2024-05-23T20:04:17.216377Z","shell.execute_reply.started":"2024-05-23T20:04:16.960276Z","shell.execute_reply":"2024-05-23T20:04:17.215004Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.barplot(x=feature_scores.values, y=feature_scores.index, palette=\"viridis\")\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title('Feature Importance Scores')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:17.218017Z","iopub.execute_input":"2024-05-23T20:04:17.218420Z","iopub.status.idle":"2024-05-23T20:04:17.672040Z","shell.execute_reply.started":"2024-05-23T20:04:17.218389Z","shell.execute_reply":"2024-05-23T20:04:17.670774Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we see the feature importance scores plotted in a bar chart. Response is the most important feature, by far, for predicting the recurrence of thyroid cancer.","metadata":{}},{"cell_type":"markdown","source":"## 2.4 Permutation Importance","metadata":{}},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance\n\n# Determine permutation importance\nperm_importance = permutation_importance(clf, X_test, y_test, n_repeats=30, random_state=0)\n\n# Create a DataFrame for permutation importances\nperm_importances = pd.DataFrame(perm_importance.importances, index=X_test.columns).T\n\n# Convert permutation importance results into a DataFrame\nperm_importance_df = pd.DataFrame(perm_importance.importances_mean, index=X_test.columns, columns=['Importance'])\nperm_importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n\nprint(\"\\nPermutation Importances:\")\nprint(perm_importance_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:17.673657Z","iopub.execute_input":"2024-05-23T20:04:17.674093Z","iopub.status.idle":"2024-05-23T20:04:22.900877Z","shell.execute_reply.started":"2024-05-23T20:04:17.674053Z","shell.execute_reply":"2024-05-23T20:04:22.899535Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the permutation importance using box plots\nplt.figure(figsize=(12, 8))\nsns.boxplot(data=perm_importances, orient='h')\nplt.title('Permutation Importances of Features')\nplt.xlabel('Decrease in Accuracy')\nplt.ylabel('Feature')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:22.902530Z","iopub.execute_input":"2024-05-23T20:04:22.902988Z","iopub.status.idle":"2024-05-23T20:04:23.480566Z","shell.execute_reply.started":"2024-05-23T20:04:22.902944Z","shell.execute_reply":"2024-05-23T20:04:23.479199Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Response has the greatest effect on accuracy, and is by far the most relevant feature in terms of permutation importance.","metadata":{}},{"cell_type":"markdown","source":"# 3. Building and Evaluating a Model","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Selecting a Classification Model","metadata":{}},{"cell_type":"markdown","source":"I'm going to create an empty list model_scores and use a for loop to cycle through 8 popular classification models using similar parameters, and then I will determine the best model depending on the Accuracy, F1 Score, and Precision. Shoutout to @abdmental01 for sharing this technique!","metadata":{}},{"cell_type":"code","source":"# # Create a dictionaries of list of models to evaluate performance with hyperparameters\nmodels = { \n          'LogicRegression' : (LogisticRegression(), {}),\n          'SVC' : (SVC(), {'kernel': ['rbf', 'poly', 'sigmoid']}),\n          'DecisionTreeClassifier' : (DecisionTreeClassifier(random_state=42), {'max_depth': [None, 5, 10],'random_state': [42]}),\n          'RandomForestClassifier' : (RandomForestClassifier(random_state=42), {'n_estimators': [10, 100],'random_state': [42],'max_depth': [None, 5, 10]}),\n          'KNeighborsClassifier' : (KNeighborsClassifier(), {'n_neighbors': np.arange(3, 100, 2),}),\n          'GradientBoostingClassifier' : (GradientBoostingClassifier(random_state=42),{'n_estimators': [10, 100],'random_state': [42]}),\n          'XGBClassifier' : (XGBClassifier(), {'n_estimators': [10, 100]}),  \n          'AdaBoostClassifier': (AdaBoostClassifier(random_state=42), {'n_estimators': [10, 100],'random_state': [42]}),\n          }\n\n# train and predict each model with evaluation metrics as well making a for loop to iterate over the models\nmodel_scores = []\nfor name, (model, params) in models.items():\n    # create a pipline\n    pipeline =RandomizedSearchCV(model, params, cv=5)\n    \n    # fit the pipeline\n    pipeline.fit(X_train, y_train)\n    \n    # make prediction from each model\n    y_pred = pipeline.predict(X_test)\n    # Metric\n    accuracy = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred,average='macro')\n    precision = precision_score(y_test, y_pred,average='macro')\n    best_parameter = pipeline.best_params_\n    # Append these in the model_scores\n    model_scores.append((name,accuracy , f1 , precision,best_parameter))                                    \n\n# selecting the best model from all above models with evaluation metrics sorting method\nsorted_models = sorted(model_scores, key=lambda x: x[1])\n\n# Printing Each model with evaluation metrics\nprint('Accuracy Of Models')\nfor model in sorted_models:\n    print('Accuracy_score for', f\"{model[0]} is {model[1]: .2f}\")\nprint('\\n')\nprint('F1 Score Of Models')\nfor model in sorted_models:\n    print('F1_Score for', f\"{model[0]} is {model[2]: .2f}\")\nprint('\\n')\nprint('Precision Of Models')\nfor model in sorted_models:\n    print('Precision for', f\"{model[0]} is {model[3]: .2f}\")\n# Selecting the best model based on MAE\nbest_Accuracy_model = max(model_scores, key=lambda x: x[1])\nprint(f\"Best model based on Accuracy is {best_Accuracy_model[0]} with Accuracy of {best_Accuracy_model[1]:.2f}\")\n\n# Selecting the best model based on R2\nbest_f1_model = max(model_scores, key=lambda x: x[2])\nprint(f\"Best model based on F1_Score is {best_f1_model[0]} with F1_Score of {best_f1_model[2]:.2f}\")\n\n# Selecting the best model based on MSE \nbest_Precision_model = max(model_scores, key=lambda x: x[3])\nprint(f\"Best model based on Precision is {best_Precision_model[0]} with Precision of {best_Precision_model[3]:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:23.482333Z","iopub.execute_input":"2024-05-23T20:04:23.482720Z","iopub.status.idle":"2024-05-23T20:04:31.475720Z","shell.execute_reply.started":"2024-05-23T20:04:23.482688Z","shell.execute_reply":"2024-05-23T20:04:31.474261Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I can see that a RandomForestClassifier or DecisionTreeClassifier will likely result in the greatest accuracy on this dataset. I'll try both out and select the model which performs best.","metadata":{}},{"cell_type":"markdown","source":"## 3.2 Building a Decision Tree Model","metadata":{}},{"cell_type":"code","source":"dtc = DecisionTreeClassifier(max_depth = 10, random_state=42)\ndtc.fit(X_train, y_train)\n\ny_pred = dtc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:31.477425Z","iopub.execute_input":"2024-05-23T20:04:31.477886Z","iopub.status.idle":"2024-05-23T20:04:31.492394Z","shell.execute_reply.started":"2024-05-23T20:04:31.477828Z","shell.execute_reply":"2024-05-23T20:04:31.491068Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Decision Tree Model Evaluation","metadata":{}},{"cell_type":"code","source":"predictions = dtc.predict(X_test)\nscore = round(accuracy_score(y_test, predictions), 3)\ncm1 = cm(y_test, predictions)\nsns.heatmap(cm1, annot=True, fmt=\".0f\")\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Accuracy Score: {0}'.format(score), size = 15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:31.493978Z","iopub.execute_input":"2024-05-23T20:04:31.495126Z","iopub.status.idle":"2024-05-23T20:04:31.757582Z","shell.execute_reply.started":"2024-05-23T20:04:31.495090Z","shell.execute_reply":"2024-05-23T20:04:31.756329Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 Building a Random Forest Model","metadata":{}},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=10, max_depth = 10, random_state=42)\nrfc.fit(X_train, y_train)\n\ny_pred = rfc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:31.759284Z","iopub.execute_input":"2024-05-23T20:04:31.759662Z","iopub.status.idle":"2024-05-23T20:04:31.800022Z","shell.execute_reply.started":"2024-05-23T20:04:31.759628Z","shell.execute_reply":"2024-05-23T20:04:31.798768Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.5 Random Forest Model Evaluation","metadata":{}},{"cell_type":"code","source":"predictions = rfc.predict(X_test)\nscore = round(accuracy_score(y_test, predictions), 3)\ncm1 = cm(y_test, predictions)\nsns.heatmap(cm1, annot=True, fmt=\".0f\")\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Accuracy Score: {0}'.format(score), size = 15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:31.801252Z","iopub.execute_input":"2024-05-23T20:04:31.801573Z","iopub.status.idle":"2024-05-23T20:04:32.247159Z","shell.execute_reply.started":"2024-05-23T20:04:31.801546Z","shell.execute_reply":"2024-05-23T20:04:32.245970Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred, average='macro')\nprecision = precision_score(y_test, y_pred, average='macro')\nprint(f'Accuracy of the model is: {accuracy}')\nprint(f'F1 score of the model is: {f1}')\nprint(f'Precision of the model is: {precision}')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:32.248926Z","iopub.execute_input":"2024-05-23T20:04:32.249416Z","iopub.status.idle":"2024-05-23T20:04:32.265201Z","shell.execute_reply.started":"2024-05-23T20:04:32.249374Z","shell.execute_reply":"2024-05-23T20:04:32.263850Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Conclusion","metadata":{"execution":{"iopub.status.busy":"2024-05-23T05:08:08.907720Z","iopub.execute_input":"2024-05-23T05:08:08.908566Z","iopub.status.idle":"2024-05-23T05:08:08.914041Z","shell.execute_reply.started":"2024-05-23T05:08:08.908512Z","shell.execute_reply":"2024-05-23T05:08:08.912803Z"}}},{"cell_type":"markdown","source":"* A Random Forest Classifier is the most effective model to predict recurrence of Thyroid cancer using this dataset\n\n* The most important feature in determining this calculation is 'Response', followed by 'Risk'\n\n* The Random Forest model we chose is 98.3% accurate when compared to the test set\n\n* The F1 score of the Random Forest Model is 97.8%\n\n* The precision of the Random Forest Model is 97.8%\n\n* This is a relatively small dataset (383 entries) and I would love to see how the model fairs on a larger slice of data","metadata":{}},{"cell_type":"markdown","source":"# Bonus: What Happens if we Drop the Most Important Feature?","metadata":{}},{"cell_type":"markdown","source":"I'm going to drop 'Response' from the training and testing sets and see how our model responds to losing the most important feature.","metadata":{}},{"cell_type":"code","source":"X_train = X_train.drop('Response', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:32.266855Z","iopub.execute_input":"2024-05-23T20:04:32.267351Z","iopub.status.idle":"2024-05-23T20:04:32.274854Z","shell.execute_reply.started":"2024-05-23T20:04:32.267309Z","shell.execute_reply":"2024-05-23T20:04:32.273626Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = X_test.drop('Response', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:32.276909Z","iopub.execute_input":"2024-05-23T20:04:32.277430Z","iopub.status.idle":"2024-05-23T20:04:32.288840Z","shell.execute_reply.started":"2024-05-23T20:04:32.277396Z","shell.execute_reply":"2024-05-23T20:04:32.287522Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the classifier with n_estimators = 100\nclf = RandomForestClassifier(n_estimators=10, max_depth = 10, random_state=42)\n\n# fit the model to the training set\nclf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:32.290265Z","iopub.execute_input":"2024-05-23T20:04:32.290680Z","iopub.status.idle":"2024-05-23T20:04:32.335484Z","shell.execute_reply.started":"2024-05-23T20:04:32.290648Z","shell.execute_reply":"2024-05-23T20:04:32.334305Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I want to see the permutation importance of features with 'Response' removed.","metadata":{}},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance\n\n# Determine permutation importance\nperm_importance = permutation_importance(clf, X_test, y_test, n_repeats=30, random_state=0)\n\n# Create a DataFrame for permutation importances\nperm_importances = pd.DataFrame(perm_importance.importances, index=X_test.columns).T\n\n# Convert permutation importance results into a DataFrame\nperm_importance_df = pd.DataFrame(perm_importance.importances_mean, index=X_test.columns, columns=['Importance'])\nperm_importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n\nprint(\"\\nPermutation Importances:\")\nprint(perm_importance_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:32.336771Z","iopub.execute_input":"2024-05-23T20:04:32.337181Z","iopub.status.idle":"2024-05-23T20:04:33.952159Z","shell.execute_reply.started":"2024-05-23T20:04:32.337121Z","shell.execute_reply":"2024-05-23T20:04:33.950868Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the permutation importance using box plots\nplt.figure(figsize=(12, 8))\nsns.boxplot(data=perm_importances, orient='h')\nplt.title('Permutation Importances of Features')\nplt.xlabel('Decrease in Accuracy')\nplt.ylabel('Feature')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:33.953645Z","iopub.execute_input":"2024-05-23T20:04:33.954007Z","iopub.status.idle":"2024-05-23T20:04:34.481717Z","shell.execute_reply.started":"2024-05-23T20:04:33.953975Z","shell.execute_reply":"2024-05-23T20:04:34.480485Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I can see now that risk is the most important feature. Note that none of these features have close to the same effect on decrease in accuracy that 'Response' had earlier.\n\nI'll fit a random forest model to the adjusted training sets.","metadata":{}},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=10, max_depth = 3, random_state=42)\nrfc.fit(X_train, y_train)\n\ny_pred = rfc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:34.483312Z","iopub.execute_input":"2024-05-23T20:04:34.483680Z","iopub.status.idle":"2024-05-23T20:04:34.522685Z","shell.execute_reply.started":"2024-05-23T20:04:34.483649Z","shell.execute_reply":"2024-05-23T20:04:34.521633Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'll evaluate the accuracy of the model visualized again in a confusion matrix.","metadata":{}},{"cell_type":"code","source":"predictions = rfc.predict(X_test)\nscore = round(accuracy_score(y_test, predictions), 3)\ncm1 = cm(y_test, predictions)\nsns.heatmap(cm1, annot=True, fmt=\".0f\")\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Accuracy Score: {0}'.format(score), size = 15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:34.524203Z","iopub.execute_input":"2024-05-23T20:04:34.524586Z","iopub.status.idle":"2024-05-23T20:04:34.784707Z","shell.execute_reply.started":"2024-05-23T20:04:34.524550Z","shell.execute_reply":"2024-05-23T20:04:34.783513Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred, average='macro')\nprecision = precision_score(y_test, y_pred, average='macro')\nprint(f'Accuracy of the model is: {accuracy}')\nprint(f'F1 score of the model is: {f1}')\nprint(f'Precision of the model is: {precision}')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:04:34.786306Z","iopub.execute_input":"2024-05-23T20:04:34.786875Z","iopub.status.idle":"2024-05-23T20:04:34.803073Z","shell.execute_reply.started":"2024-05-23T20:04:34.786832Z","shell.execute_reply":"2024-05-23T20:04:34.801901Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy of the model drops (expectedly) but at 89.6% this is still a respectable model. We could somewhat reasonably predict the recurrence of thyroid cancer in absence of knowing the response to cancer treatment.","metadata":{}}]}